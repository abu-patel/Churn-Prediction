---
title: "Churn Prediction - A new Scientific Approach"
author: "Abuzar Patel - Data Science Internship"
date: '2023-08-23'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstarct

*Machine Learning, B2B, Time Slicing*
Our Churn Analysis aims to contribute to both the theoretical and empirical body of knowledge in the non-contractual **B2B** customer churn prediction domain. In particular, we explore: (i) whether it is possible to use a single common source of business data (i.e., invoice data) to devise predictive models capable of reliably identifying churners in real-world settings, (ii) the effects of using different amounts of historical data for devising features on the performance of resulting models, and (iii) whether using alternative churn definitions could yield models that perform well enough to serve as foundations for discussing new potential retention activities.

Finally, by leveraging a recently proposed approach to training data set creation and comparing it with the approach used traditionally, we aim to evaluate whether it generalizes to different case data.

## Churn Definition:

Churn Definition: Churn can be defined in multiple ways however the first step is the **timeline** to consider. Let's say one is interested in predicting quarterly churn then churn at a customer level can be defined as customers who did not make any purchase in this quarter but did in previous quarter. Then this customer can be addressed with a value of 1(Churn) otherwise 0 or non-churn. Similarly, one can define churn yearly, monthly and at times even weekly depending on business needs. Our model is capable of handling churn at any time level, and this is only one of its robust capabilities :

## What sets this model so apart from traditional models?:

Customer churn prediction models using machine learning classification have been developed predominantly by training and testing on one time slice of data. We train models on multiple time slices of data and refer to this approach as **multi-slicing**. Our results show that given the same time frame of data, multi-slicing significantly improves churn prediction performance compared to training on the entire data set as one time slice. We demonstrate that besides an increased training set size, the improvement is driven by training on samples from different time slices. We confidently show that multi-slicing addresses the rarity of churn samples and the risk of over fitting to the distinctive situation in a single training time slice. Multi-slicing makes a model more generalization, which is particularly relevant whenever conditions change or fluctuate over time. Please contact me to discuss how to choose the number of time slices. This approach is robust, addresses seasonality in purchasing pattern, handles missing trend values, focuses on out of period testing rather than traditional out of sample testing and from an imbalance response(churn) distribution modelling standpoint this approach is neither an oversample nor under sample or cost sensitive learning, instead we represent the data for what it is, with each time slice as most recent time to ensure our approach is as close to reality as possible, thereby reducing overfitting. Moreover, as time increases so does the number of training slices which would result in better model performance by leveraging each observation to predict Churn. In order to maintain no data leakage between multiple training slices and test set we have developed a methodology that uses 2X5 nested cross validation where the inner loop tunes the hyper parameter along with a wrapper method for feature selection and the outer loop delivers performance metrics on the whole set. Personally, an achievement that we are very proud of is that only by using invoice data mainly we can come up with a set of 52 feature space that can be applied to literally any distributor and when integrated with the power of multi-slice approach and statistically sound machine learning techniques we can significantly bridge the gap between current limitations in non-contractual B2B setting.

## Time Slicing Technique -

![MultiSlice Methodology\
](images/Tme_Slice.png)

![MultiSlice Visulaization](images/Picture2-TS.png)

**Advantages of MultiSlice Technique:**

-   It is robust enough to account for existing seasonality

-   The multislice model is neither an over sample nor an under-sample approach when it comes to imbalanced response distribution. Because of multiple training slices the model simply represent the data for what it is and ultimately tackles the class imbalance issue

-   Instead of out of sample testing the multislice model considers out of period testing that is testing on a more recent time slice which is much more realistic than random sampling.

-   Increase the training size drastically. Depending on number of years of data at hand we have methodologies in place to determine the right number of training slices. This dramatically increase the training size leveraging as much data as possible

-   Missing Trend values are implicitly calculated by the model as the different training slices are a result from shift in time and therefore eliminates the need of explicitly calculating trends

## Feature Space

Reading from my excel file: Please note the data is given in the github and can be accessed accordingly

```{r}
f<-"C:\\Users\\patel\\OneDrive\\Desktop\\ValleySolventsChurnDataMultiSlice_feat.xlsx"
library(readxl) #package to read from excel
library(dplyr)  #package for data manipulation 
data<- read_excel(f,na="TRUE") #reading file from excel
names(data) #checking column names
dim(data) #checking dimension  how many rows and columns
```

Note: I am not going into detail of what the EDA was like, as I really want to focus on the heart of the project which is an **innovative transformation of data into a multi time slice approach**. I truly believe EDA is something most people are familiar with and when and if needed please feel free to run this code and expand upon this analysis. Data can be found in my github rep--

## Response Variable:

our response variable in this case is the column "Churn"

```{r}
tail(data$Churn) # 0 means nor churn and 1 means Churn
```

## 2X5 NESTED CROSS VALIDATION MultiTime Slice Technique:

For statistically testing our results, we derive a technique for conducting cross-validation with out-of-period testing while using multiple time slices within the training set. We consider two-fold cross-validation. The technique entails partitioning the data set into two equally sized subsets or folds which are each used once for testing.

In each iteration j of cross-validation, it is essential that the subsets of customers used for training Cj(t\^train) and testing Cj(t\^train) are not overlapping. It is not straightforward how to ensure this when applying out-of-period testing since different observations of the same customers can appear in both training and testing time slice.

![](images/paste-A0292DDA.png)

We illustrate our approach for two-fold cross-validation in Fig below.

The goal is to use each observation only once for testing, either in iteration 1 or in iteration 2 and to never train on customers that are also used for testing in the same iteration. The testing slice customers are assigned to two equally sized disjoint testing folds C1(t\^test) and C2(t\^test). The training slice customers are also divided into subsets, ensuring that customers used for testing in the same iteration are not included in the corresponding training subset. Customers in C1(t\^test) that also appear in the (older) training slice are assigned to the opposite fold for training C2(t\^train) and vice versa. Customers that only appear in the training slice are randomly assigned to the two folds. In the first iteration, a model is trained on C1(t\^train) and tested on C1(t\^test) and in the second iteration, it is trained on C2(t\^train) and tested on C2(t\^test). The subset of customers trained on and the subset of customers tested on are disjoint in each iteration: C1(t\^train) ∩ C1(t\^test) = ∅ and C2(t\^train) ∩ C2(t\^test) = ∅.

We extend this to multi-slicing so that for each customer included in a testing fold, all previous observations of the customer are excluded from the corresponding training fold (see Fig. 4). Customers that only appear in the training set are randomly assigned to one of the two training folds. The approach generates disjoint subsets of customers for training and testing in each iteration:

![](images/paste-C61562F1.png)

## Data Processing and setting up the logic

-   Domain knowledge helps here, as for example in code below we need to eliminate every observations that only had one single hit. Hit is a variable - which means how many times did the customer order in the past 9 months of training data. If customer has ordered only once, makes sense to remove that customer,

```{r}
data[which(is.na(data$Hits)),"Hits"]=0   #Checking if hits is 0 or not and if yes what are the indices

#remove single hit customers
data <- data[-which(data$Hits<=1),]
dim(data) #checking dimensions again, 
```

-   Selecting training set from original data set. In the pre processing phase, I came up with this 54 variables from scratch and added column (variable) called sheet name, which gives direction to training and testing selection.

```{r}
# now selecting only the training set without the testing from the original data set. 
train_data<-data %>%
  filter(SheetName != "Test")

dim(train_data) # always checking dimensions to ensure number of column is preserved

```

-   Handling Missing values from train and test:
-   Reasonable assumption below as we don't have much missing data.

```{r}
#seeing what are the mean values for column that have missing data, The means are without the missing data. (We use these values and some additional logic to replace missing data) 
na_col_train<-colnames(train_data)[colSums(is.na(train_data))>0]
na_col_train # these column have missing data

grps<- train_data%>%
  group_by(Churn)%>%
  summarize(Gm=mean(GMPercent,na.rm=T),Time=mean(AvgTimeDiff,na.rm=T),CV=mean(CoV_Sales,na.rm=T))
```

-   Selecting Testing data (similar to training)

```{r}
test<- data%>%
  filter(SheetName=="Test")
dim(test) #checking dimension of testing data

#checking na column in entire data 
na_col<-colnames(test)[colSums(is.na(test))>0]
na_col


#replacing the NA values without changing the order in the test data. similar to training. or you can eliminate the missing data, it yields very similar result

test[which(is.na(test$Hits<=2 & test$AvgTimeDiff)),"AvgTimeDiff"] = grps$Time[2] 
test[which(is.na(test$Hits<=2 &  test$CoV_Sales)),"CoV_Sales"] = grps$CV[2]
test[which(is.na(test$Hits == 2 & test$GMPercent)),"GMPercent"] = grps$Gm[2]
test[which(is.na(test$Hits!=2 & test$AvgTimeDiff)),"AvgTimeDiff"] = grps$Time[1]
test[which(is.na(test$Hits!=2 &  test$CoV_Sales)),"CoV_Sales"] = grps$CV[1]
test[which(is.na(test$GMPercent != 2 & test$GMPercent)),"GMPercent"] = grps$Gm[1]

#checking if test still contains na or not.... should not contain NA anymore
na_col<-colnames(test)[colSums(is.na(test))>0]
#confirmed 
na_col
```

-   CARET library for model building:

```{r}
library(caret) # caret library for preprocessing, model building, confusion matrix
```

-   Dividing Test data into two disjoint sets, preparing for cross validation

```{r}
#dividing the test set into two equally disjoint sets. We don't need to shuffle the data because objective is to test on a more recent time 

#before dividing checking if any of the customer ID will overlap in any of the future test fold(see code below this). 
#we don't want the customer ID to exist multiple time in the two testing set because we need to test each customer once and only once !!!


# logic, if number of observation is even then just half of the total number of obs else if odd then round 
if (dim(test)[1] %% 2 == 0) {
  l<- dim(test)[1]/2
}else {
  l<- round(0.5*dim(test)[1])
}
l # number of row where splitting will happen 

nrow(test) #total number of row
# testing_folds$Fold1
intersect(test[1:l,]$customerID,test[l+1:nrow(test),]$customerID) # confirming if disjoint or not 
test_fold1<-test[1:l,] #creating test fold 1 
test_fold2<-test[(l+1):nrow(test),] #creating test fold 2 
#this proves that customerID exist only once in each of the testing folds
#this proves that customers are disjoint in both folds
intersect(test_fold1$customerID,test_fold2$customerID)

```

-   Now, we will process Training Data and prepare it for a 2X5 Nested Cross Validation Technique:

```{r}
#now we have train data to consider

names(train_data)

#to be sure checking which column in the training set has missing values and replace it based on the response value by grouping them, check line 80
na_train<- colnames(train_data)[colSums(is.na(train_data))>0]
na_train
#replacing na values in training data 

train_data[which(is.na(train_data$Churn==0 & train_data$AvgTimeDiff)),"AvgTimeDiff"]= grps$Time[1]
train_data[which(is.na(train_data$Churn==0 & train_data$GMPercent)),"GMPercent"]= grps$Gm[1]
train_data[which(is.na(train_data$Churn==0 & train_data$CoV_Sales)),"CoV_Sales"]= grps$CV[1]
train_data[which(is.na(train_data$Churn==1 & train_data$AvgTimeDiff)),"AvgTimeDiff"]= grps$Time[2]
train_data[which(is.na(train_data$Churn==1 & train_data$GMPercent)),"GMPercent"]= grps$Gm[2]
train_data[which(is.na(train_data$Churn==1 & train_data$CoV_Sales)),"CoV_Sales"]= grps$CV[2]

#checking if na still exist in train data
# colnames(train_data)[colSums(is.na(train_data)>0)]
#NO NAS found ^^






#making index for those customers that are unique only in training data when compared to entire test data 
only_train_index<-which(train_data$customerID %in% test$customerID=="FALSE")
#making  separate data set for those customers that exist only in training data 
C<-train_data[only_train_index,]
#deleting those customers from original data frame to make sure we are not duplicating customers
train_data<-train_data[-only_train_index,]


```

-   Randomly splitting training set into two different training sets

```{r}
#randomly sampling those customers that are only present in train into either of the two training folds
set.seed(111)
index<-sample(1:nrow(C),size=round(0.5*nrow(C)),replace=F)
#C1_train is training fold 1
C1_train<- C[index,]
#training fold 2
C2_train<- C[-index,]
# dim(C1_train)
# dim(C2_train)
# dim(train_data)

#two random folds are created
# train_data[train_data$customerID %in% C$customerID=="TRUE",]
# r1<-which(train_data$customerID %in% C$customerID=="TRUE")
# train_data<- train_data[-r1,]

#checking intersection

# train_data$customerID %in% C$customerID -- No common found as expected


#removed customers from original train data as already placed them in two folds
# train_data[train_data$customerID %in% C$customerID=="TRUE",]
# #customers that are in training data but not in C because we need to remove these customers from original set
# dim(train_data)


#now checking the remaining customers in original training data with test fold 1

which(train_data$customerID %in% test_fold1$customerID=="TRUE")

#seeing which customers in test fold 1 also present in training data
to_fold2_train<-which(train_data$customerID %in% test_fold1$customerID=="TRUE")
#inputting the customers that are present in test 1 and training in fold 2 of training

C2_train<-rbind(train_data[to_fold2_train,],C2_train)
sum(C2_train$customerID %in% test_fold1$customerID)
sum(train_data[to_fold2_train,]$customerID %in% test_fold2$customerID)
#deleting the training values which we just placed in C2_train from original train data
train_data<-train_data[-to_fold2_train,]
dim(train_data)

```


-repeating for test set 2 

```{r}
#repeating process for test set 2 now. objective is to find index of the customers that are overlapping between remaining training data and test set 2 and taking those customer indexes and placing them in fold 1 of training (C1_train)

# checking customers that exist in original train data and test2 now.
sum(train_data$customerID %in% test_fold2$customerID) #-- # should be 0 

#index of fold1
to_fold1_train<-which(train_data$customerID %in% test_fold2$customerID=="TRUE")


#binding
C1_train<- rbind(train_data[to_fold1_train,],C1_train)

#proof that Intersection of test fold 1 and Fold 1 train is 0
sum(C1_train$customerID %in% test_fold1$customerID)
#proof that intersection of test fold 2 and train fold 2 are 0
sum(C2_train$customerID %in% test_fold2$customerID)
train_data<-train_data[-to_fold1_train,]
dim(train_data) # must be zero--- this is a proof that we started with X number of observation in training and then eliminated those as we split them in to two different training ensure no over laps with testing data for cross validation purposes

#that test fold 1 and fold 2 are disjoint set proof
sum(test_fold2$customerID %in% test_fold1$customerID)
#hence proved. 
intersect(C1_train$customerID,test_fold1$customerID) #proof of no data leakage 
intersect(C2_train$customerID,test_fold2$customerID) # proof of no data leakage

# dim(C1_train)
# dim(C2_train)
```
## Summary of processing 
-  in summary we developed a simple yet effective technique to ensure the following : 
-- the customers in testing set are tested only once in each iteration (2) and therefore  developed two disjoint sets of test fold1 and fold 2
-- then we made sure that customers in training and test set are not overlapping in each iteration
-- for example iteration one will be over C1_train and test fold 1 with no overlapping customers
-- second iteration will be over C2_train and test fold 2 with no overlapping customers 
 
 ultimate goal of doing this was we need to test on **out of period data** and not out of sample
 which in other words is very useful for time series data. Need to test on most recent time while 
 capturing and training over as much data as we can **without any overlapping customers**. 
we created two training set and two test sets to conduct a nested 5X2 cross validation where the outer loop will be used for final testing and calculating AUC Metric and inner loop to tune hyper parameters 


## Response Distribution check after processing:

-- checking imbalance in both training and both testing data, might help us to make a judgement in order to which  weights to use during our modelling. 
```{r}
prop.table(table(C1_train$Churn)) #proportion of Churn and non Churn in C1_train
table(C2_train$Churn)       
prop.table(table(C2_train$Churn)) # proportion of Churn and non Churn in C2_train
prop.table(table(test_fold1$Churn)) #proportion of Churn and non Churn in fold1 test
prop.table(table(test_fold2$Churn)) #proportion of Churn and non Churn in fold2 test
#dim(C2_train)
#dim(test_fold2)
#dim(C1_train)
#dim(test_fold1)
```
##Modelling and final data preparation

- I tried many models and weighted random forest gave the best result, for this purpose I will be demonstrating only the best model here. 

```{r finalizing data for modelling}
#deleting unwanted column now, such as sheet name and LocationName..... Because I am familiar with data set I know locationNname is not an important factor
names(C1_train)
C1_train<-C1_train %>%
  select(-SheetName,-locationName)
C1_train$Churn <- factor(C1_train$Churn) #converting Churn to a factor or a qualitative rather than character 
names(C1_train)

#making different name for response label as it is a requirement for the caret package. 1 and 0 are converted to X1 and X0
C1_train<- C1_train %>% 
  mutate(Churn = factor(Churn,labels = make.names(levels(Churn))))

#repeating the same process for the second training set and testing sets
#for C2
C2_train<-C2_train %>%
  select(-SheetName,-locationName)
C2_train$Churn <- factor(C2_train$Churn)
names(C2_train)

C2_train<- C2_train %>% 
  mutate(Churn = factor(Churn,labels = make.names(levels(Churn))))

#repeating same for test fold 1
test_fold1<- test_fold1%>%
  select(-SheetName,-locationName)
test_fold1$Churn<- factor(test_fold1$Churn)
dim(test_fold1)
dim(C1_train)
test_fold1<- test_fold1 %>% 
  mutate(Churn = factor(Churn,labels = make.names(levels(Churn))))

#for test fold 2


test_fold2<- test_fold2%>%
  select(-SheetName,-locationName)
test_fold2$Churn<- factor(test_fold2$Churn)
dim(test_fold2)

test_fold2<- test_fold2 %>% 
  mutate(Churn = factor(Churn,labels = make.names(levels(Churn))))
dim(test_fold2)
dim(test_fold1)


#initializing lists  for nested CV later
inner_list<-list()
inner_list<- list(C1_train,C2_train)
outer_list<- list(test_fold1,test_fold2)

# tuning_over_inner<- list()
# predictions_outer<- list ()
# prob_pred<- list()
library(MLmetrics)

#converting market segments into factor 
inner_list[[1]]$marketSegment<- factor(inner_list[[1]]$marketSegment)
inner_list[[2]]$marketSegment<- factor(inner_list[[2]]$marketSegment)
outer_list[[1]]$marketSegment<- factor(outer_list[[1]]$marketSegment)
outer_list[[2]]$marketSegment <- factor(outer_list[[2]]$marketSegment)


#using boruta for feature selection... just to have an idea what the features seems like. WE ALSO do RFE to select features = later on. 

#3  thing to keep in mind is that customers in the training set exist multiple times and have overlaps as we are conducting a multi time slice approach 
#features are calculated for the earliest 9 months and subsequently response labels are calculated three months from that. then we make a new training set by conducting a one month shift 
#we do this until we have exhausted the 2 years worth of data. 
#however the testing set ensures a three month shift rather than a month as we don't want any information leakage between training and testing. 
#in short, in the end, we end up with 10 different training data each with a one month shift after initial 9 months of data used to develop the feature space 
#and testing set with a three month shift. the feature space window is 9 month and response or churn label is three months as we need to predict quarterly churn. 

library(Boruta)
boruta<- Boruta(Churn~., data = outer_list[[1]][,-1],doTrace=2,maxRuns=300)
print(boruta)
plot(boruta,las=2,cex.axis=0.7)
boruta$finalDecision

attStats(boruta)
getNonRejectedFormula(boruta) #taking only those that relevant from boruta 
teb<-TentativeRoughFix(boruta)
teb$finalDecision


getNonRejectedFormula(teb) #this gives you the formula that goes into training

#code for recursive feature elimination but very time consuming and computational heavy 
# m_rfe<-list()
# library(rsample)
# set.seed(20140102)
# 
# 
# 
# for (i in 1:2) {
#   rfFuncs$summary <- twoClassSummary
#   subs <- c(1:49)
#   ctrl_rfe <- rfeControl(functions = rfFuncs,method = "cv", number = 5,
#                          verbose=T)
#   
#   
#   m_rfe[[i]] <- rfe(Churn ~., data = inner_list[[i]][,-1],
#                     metric = "ROC",
#                     sizes=subs,
#                     rfeControl = ctrl_rfe)
# }
# 
# m_rfe[[1]]
# m_rfe[[2]]
# m_rfe[[1]]$bestSubset
# 
# predictors(m_rfe[[2]])
# best.set<- predictors(m_rfe[[2]])
# 
# inner_list_best<- list()
# inner_list_best[[1]]<-inner_list[[1]]%>%
#   select(best.set)
# inner_list_best[[2]]<-inner_list[[2]]%>%
#   select(best.set[[2]])
# names(inner_list[[1]])




#initializing other lists 
#update inner lists with best subset and churn value 
# do same for outer lists




ranger_fit<- list()
predict_final<- list()
predict_final_prob<- list()
auc_list<- list()
outer_model<- list()
library(ROCR) #package for ROC curve 
library(pROC) #package for AUC 
```

## Nested CV
```{r training}
set.seed(121)
#model weights is important and judgment must be used to set a number for rare class. in this case 0.15 for positive class and 0.85 for rare. 
#the number should add to one. the higher the rare class weight the better will be churn accuracy but both accuracy shold reasonably be good for a great model. 
#therefore trial and error or a good balance somewhere is required
#in cases of extreme imbalance higher weights for rare class must be considered. 
for (i in 1:2){
  model_weight<- ifelse(inner_list[[i]]$Churn=="X0",
                        (1/table(inner_list[[i]]$Churn)[1])*0.30,
                        (1/table(inner_list[[i]]$Churn)[2])*0.70)
  
  dim(data)
  #tuning for best value of mtry for AUC metric. #mtry is one of the hyper parameters of RF model
  set.seed(121)
  grid <- expand.grid(
    .mtry = c(3,5,7,14,20), #mtry cannot be greater than the number of variables
    .splitrule = "gini",
    .min.node.size = c(5,10,15,20) #tuning for minimum node size, another hyperparameter for the model
  )
  
  
  fitControl <- trainControl(method = "CV",
                             number = 5,
                             verboseIter = TRUE,
                             classProbs = TRUE,summaryFunction=prSummary)
  
  #fitting the model here 
  ranger_fit[[i]] = train(
    Churn ~ Sales + COGS + DaysToPay + GM + Hits + Recency + PurchaseSpan + 
    AvgTimeDiff + Orders_Mean + M3_Order + M2_Sales + M6_Sales + 
    M6_Order + M9_Sales + M9_Order + Q3_Sales + Q2_Sales + Q1_Order + 
    Q3_Order + Q2_Order + `#Orders` + DaysLate,
    data = inner_list[[i]][,-1],
    method = 'ranger',
    tuneGrid = grid, #using grid to tune hyperparameter 
    weights = model_weight, #passing model weights
    num.trees=1000, #number of trees. Usually a large number so that the error rate settles down
    importance="permutation", #variable importance using permutation method. for more info read embedded feature selection methods
    trControl = fitControl #controls the flow, such as CV, number of times, probability or not, etc
  )
  
  #this is control for outer loop 
  outerCtrl<- trainControl(classProbs = T,summaryFunction = prSummary,verboseIter = T)
  #grids here are taking values from inner loop and fitting again on entire model. 
  outer_grid<- expand.grid(
    .mtry = c(ranger_fit[[i]]$bestTune$mtry),
    .splitrule = "gini",
    .min.node.size = c(ranger_fit[[i]]$bestTune$min.node.size))
  
  
  outer_model[[i]]<-train(Churn ~ Sales + COGS + DaysToPay + GM + Hits + Recency + PurchaseSpan + AvgTimeDiff + Orders_Mean + M3_Order + M2_Sales + M6_Sales + 
    M6_Order + M9_Sales + M9_Order + Q3_Sales + Q2_Sales + Q1_Order + 
    Q3_Order + Q2_Order + `#Orders` + DaysLate, data = inner_list[[i]][,-1],
                          method="ranger",
                          tuneGrid= outer_grid,
                          num.trees = 1500,
                          importance = "permutation",
                          weights = model_weight,
                          trControl = outerCtrl)
  
  # don't forget to initialize the lists
  #predicting churn values 
  predict_final[[i]]<- predict(outer_model[[i]],outer_list[[i]])
  #predicting churn probability values
  predict_final_prob[[i]]<- predict(outer_model[[i]],outer_list[[i]],type="prob")
  #calculating AUC metric
  auc_list[[i]]<- auc(outer_list[[i]]$Churn, predict_final_prob[[i]][,2])
  
}
```



## Prediction Phase :

```{r Prediction  ROC and confMatrix}

# v<-predict(ranger_fit[[1]],outer_list[[1]])
# confusionMatrix(v,outer_list[[1]]$Churn)
# auc_list[[1]]
# auc_list[[2]]


#confusion matrix for both sets now
confusionMatrix(predict_final[[1]],outer_list[[1]]$Churn)$byClass
confusionMatrix(predict_final[[2]],outer_list[[2]]$Churn)$byClass



#ROC CURVE for 1 and 2 data sets
library(caTools)
# for 1st 
xx<-predict_final_prob[[1]]$X0
yy<-outer_list[[1]]$Churn
as.numeric(yy)
colAUC(xx,yy,plotROC = TRUE)

#for 2nd
xx2<-predict_final_prob[[2]]$X0
yy2<-outer_list[[2]]$Churn
as.numeric(yy2)
colAUC(xx2,yy2,plotROC = TRUE)
```
We can see an ROC score of **0.81** for both data sets, an evidence of how well we were able to model for both, without bias and data leakage. This score is considered really good for a real world problem and keeping in mind that we only used invoice data and developed these 54 features from scratch. A good Machine Learning model involves innovating data transformation and multi time slice approach is an excellent example of that

#binding into one single result instead of two data sets 
```{r}
one<-cbind(outer_list[[1]],predicted_churn=predict_final[[1]],prob=predict_final_prob[[1]])

#same for two 
two<- cbind(outer_list[[2]],predicted_churn=predict_final[[2]],prob=predict_final_prob[[2]])

#combining one and two to get complete data. think of adding the CF matrix of one and two.
#we can produce one single CF matrix now

final<-rbind(one,two)

confusionMatrix(final$predicted_churn,final$Churn)$table
confusionMatrix(final$predicted_churn,final$Churn)$byClass
```
# filtering to see potential dollar value savings
```{r Highvalue savings}
final%>%
  filter(Churn==predicted_churn & Sales> 30000 & Churn == "X1")%>%
  summarize(sum(Sales))
```
Here you can see the sum of sales of high value customer that had past sales of greater than 30000 dollars for which our model correctly predicted the churn YES...Nearly about **330K**

```{r Overall Savings on Churn Yes}
final%>%
  filter(Churn==predicted_churn  & Churn == "X1")%>%
  summarize(sum(Sales))
```
Here you can see the value we can potentially save from working with sales team and not allowing this customers to churn by giving them discounts etc and developing targeted strategies.. About **1 Million Dollars**

```{r Incorrect Predictions}
final%>%
  filter(Churn!=predicted_churn & predicted_churn =="X0")%>%
  summarize(sum(Sales))

```


```{r Combining Importance}
one_imp<-data.frame(importance1 = outer_model[[1]]$finalModel$variable.importance)
two_imp<-data.frame(importance2 = outer_model[[2]]$finalModel$variable.importance)

#final importance table
importance<-cbind(one_imp,two_imp)
final_importance<-importance%>%
  mutate(mean_importance = (importance1+importance2)/2)%>%
  arrange(desc(mean_importance))%>%
  select(mean_importance)
final_importance

```
Here we have an overall Varibale importance plot to highlight the most important factors driving churn and what areas one should focus on

#plot to show the overall importance
```{r Overall Importance}
ggplot(data=data.frame(order(final_importance,decreasing=TRUE)),aes(final_importance$mean_importance,factor(rownames(final_importance)),fill=factor(rownames(final_importance))))+geom_col()

final_importance%>%
  mutate(perc=mean_importance/sum(mean_importance))
plot(varImp(outer_model[[1]]))
plot(varImp(outer_model[[2]]))

```
We can average the variable importance score from both model and rank them, however I wanted to highlight how the model was able to detect similar feature importance for both the models. Clearly Purchase Span and Avg Time Difference between orders is the most important features


